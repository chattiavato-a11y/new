# WebLLM + WebML Memory + Escalation Plan

## 1. Default Fallback & Escalation Loop
1. **Client-side KB:** `services/fallback_kb.js` ships an OPS-aligned bilingual knowledge base. `window.FallbackKB.reply()` now powers the greeting, offline answers, and the default message whenever the API returns no/low confidence.
2. **Confidence Gate:** The web client tags every assistant response with `confidence` metadata. If `confidence < 0.55`, `reply` is empty, or `/api/chat` explicitly requests escalation, the UI routes the user message through the fallback KB.
3. **Cloudflare Escalation:** Each fallback triggers `POST https://withered-mouse-9aee.../fallback/escalate`. The worker validates integrity headers, accepts the JSON payload, optionally forwards it to `ESCALATION_WEBHOOK`, and returns `{ escalated: true }`. This creates an auditable chain (browser → CF Worker → downstream webhook/pager) for degraded responses.

## 2. Training Memory Loop
1. **Conversation Vault:** The browser persists up to 40 recent turns into `localStorage` (`chattia.memory.v1`). On load we hydrate the log, so the agent can “remember” the last session even if the page reloads.
2. **Structured Metadata:** Every POST `/api/chat` now contains `training_memory` (the last 6 turns) and metadata fields (`memory_key`, `memory_window`, `confidence_threshold`). The server or Tiny LLM can inspect these to personalize replies and tune the model.
3. **Feedback Telemetry:** Escalation payloads include `conversationTail`, the fallback text, and a timestamp. This gives the training loop concrete examples of low-confidence outputs to prioritize in reinforcement or supervised fine-tuning.
4. **Privacy Controls:** Because the vault lives entirely in-browser, users can clear their history by wiping local storage; nothing is synced until a signed API call explicitly transmits it.

## 3. WebLLM & WebML Enablement Steps
1. **Model Selection:** Use [WebLLM](https://mlc.ai/web-llm/) to load a 4–8B parameter Tiny LLM variant (e.g., `vicuna-v1.5-7b-q4f16_1`). For budget hardware, stage a WebML graph for retrieval/embedding via ONNX Runtime Web or WebNN.
2. **Loader Integration:** Lazy-load the WebLLM bundle after the first successful cloud response or when offline is detected. Cache the compiled kernels using the Service Worker to avoid recompile delays.
3. **Hybrid Routing:** Introduce a routing policy: (a) Cloud API handles high-confidence, security-sensitive prompts; (b) WebLLM handles quick follow-ups or offline requests; (c) fallback_kb covers deterministic FAQ answers.
4. **Memory-Augmented Prompts:** Feed the `training_memory` slice plus a distilled summary (generated by WebML embeddings + clustering) into the WebLLM prompt. This keeps the prompt window lean while respecting PCI/NIST guardrails.
5. **Continuous Improvement:** Store anonymized escalation payloads in a secure data lake. Use WebML models (e.g., MiniLM sentence transformers) to classify recurring failure modes, then refresh both the fallback KB and the WebLLM prompt templates.
6. **Compliance Hooks:** Keep all Tiny LLM artifacts under integrity control: sign model manifests, enforce CSP `default-src 'self'`, and version the WebLLM weights via Cloudflare R2 + Workers KV for rollbacks.

## 4. Cloudflare Worker Touchpoints
1. **Integrity Gateway:** `withered-mouse-9aee` remains the detached-signature mint plus escalation API. All browser calls send `X-Integrity` headers so the worker can block tampering.
2. **Escalation Webhook:** Set `ESCALATION_WEBHOOK` (Slack, PagerDuty, etc.). The worker wraps browser payloads with `{ gateway: "withered-mouse-9aee" }`, giving operations instant traceability across the OPS CyberSec Core lifecycle.
3. **Future Hooks:** Additional Worker routes can host lightweight WebLLM checkpoints (for streaming to clients) or store anonymized memory digests inside Workers KV / Durable Objects, enabling privacy-preserving personalization synchronized with the in-browser vault.
